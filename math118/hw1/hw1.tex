\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}

\title{Math 118 Homework 1}
\date{10/7/2020}
\author{Jiaping Zeng}

\newcommand{\bx}{{\bf x}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bw}{{\bf w}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\setstretch{1.35}
\maketitle

\begin{itemize}
    \item [1.] Suppose that $A\in\mathbb{R}^{m\times n}$, $\bv\in\mathbb{R}^{n}$ and $B\in\mathbb{R}^{n\times r}$. Determine the number of floating point operations required to compute $A\bv$ and $AB$.\\
          \textbf{Answer: } \begin{itemize}
              \item [$A\bv$:] Multiplying each row of $A$ to $v$ requires $n$ multiplications, then combining the products requires $n-1$ additions. Since there are $m$ rows, the total number of floating point operations is $m(n+(n-1))=2mn-m$.
              \item [$AB$:] For each column in $B$ we would need $2mn-m$ operations as shown above, repeating it for $r$ column would require $r(2mn-m)=2mnr-mr$ operations.
          \end{itemize}
    \item [2.] Let $\bv = [1,0,5,-3]^{\top}$ and $\bw = [-2,4,5,1]^{\top}$.
          \begin{itemize}
              \item [(a)] Compute $\|\bv\|_1$, $\|\bw\|_{3}$ and $\|\bv\|_{\infty}$.\\
                    \textbf{Answer: }\\
                    $\|\bv\|_1=1+0+5+-3=3$\\
                    $\|\bw\|_{3}=\sqrt[3]{(-2)^3+4^3+5^3+1^3}=\sqrt[3]{182}\approx 5.667051$\\
                    $\|\bv\|_{\infty}=\max_i\abs{v_i}=\abs{5}=5$
              \item [(b)] Verify that $\langle\bv,\bw\rangle \leq \|\bv\|_2\|\bw\|_2$ by computing both sides.\\
                    \textbf{Answer: } We have $\langle v,w\rangle=-2+0+25-3=20$ and $\|\bv\|_2\|\bw\|_2=\sqrt{1^2+6^2+(-3)^2}\cdot\sqrt{(-2)^2+4^2+5^2+1^2}=\sqrt{46}\cdot\sqrt{46}=46$, therefore $\langle\bv,\bw\rangle \leq \|\bv\|_2\|\bw\|_2$.
          \end{itemize}
    \item [3.] This question will introduce you to three different ways of thinking about matrix multiplication. For all three parts, let $U\in\mathbb{R}^{m\times n}$, $V\in\mathbb{R}^{m\times r}$ and $W\in\mathbb{R}^{r\times n}$. By $\bu_i$ (respectively $\bv_i$ and $\bw_i$) I mean the $i$-th column of $U$ (respectively $V$ and $W$).
          \begin{itemize}
              \item [(a)] Show that $\displaystyle U^{\top}V = \left[\begin{matrix} \bu_{1}^{\top}\bv_1 & \bu_{1}^{\top}\bv_2 & \ldots & \bu_{1}^{\top}\bv_{n} \\ \vdots & \ddots & \ddots & \vdots \\ \bu_{n}^{\top}\bv_{1} & \bu_{n}^{\top}\bv_{2} & \ldots & \bu_{n}^{\top}\bv_{n} \end{matrix}\right]$\\
              \textbf{Answer: } $U^{\top}V=[\bu_1,\ldots,\bu_n]^{\top}[\bv_1,\ldots,\bv_n]$
              \item [(b)] Show that $\displaystyle UW^{\top} = \sum_{i=1}^{n}\bu_{i}\bw_{i}^{\top}$.\\
              \textbf{Answer: }
              \item [(c)] Show that $U^{\top}V = [U^{\top}\bv_1, U^{\top}\bv_2,\ldots, U^{\top}\bv_{r}]$.\\
              \textbf{Answer: }
          \end{itemize}
    \item [4.] We say that a function $d: \mathbb{R}^{n}\times \mathbb{R}^{n} \to \mathbb{R}_{+}$ is a metric if:
          \begin{enumerate}
              \item $d(\bu,\bv) = d(\bv,\bu)$ for all $\bu,\bv\in\mathbb{R}^{n}$. {\em (symmetry)}
              \item $d(\bu,\bv) = 0$ if and only if $\bu = \bv$. {\em (definiteness)}
              \item $d(\bu,\bw) \leq d(\bu,\bv) + d(\bv,\bw)$ for all $\bu,\bv,\bw\in\mathbb{R}^{n}$ {\em (triangle inequality)}.
          \end{enumerate}
          Show that if $\|\cdot\|$ is a norm on $\mathbb{R}^{n}$, then $d(\cdot,\cdot)$ defined by $d(\bu,\bv) = \|\bu - \bv\|$ is a metric on $\mathbb{R}^{n}$.\\
          \textbf{Answer: } We can verify each of the properties of metric functions as follows:
          \begin{itemize}
              \item [(a)] $d(\bu,\bv)=\|\bu-\bv\|=\sum_{k=1}^n\abs{u_k-v_k}=\sum_{k=1}^n\abs{v_k-u_k}=\|\bv-\bu\|=d(\bv,\bu)$
              \item [(b)] \begin{itemize}
                        \item [$\Rightarrow$:] We have $d(\bu,\bv)=0\implies\|u-v\|=0\implies\sum_{k=1}^n\abs{u_k-v_k}=0$. Since $\abs{u_k-v_k}\geq 0$ by definition of absolute value, we must have $u_k=v_k$ for $1\leq k\leq n$. Therefore $\bu=\bv$ since each corresponding entry is identical.
                        \item [$\Leftarrow$:] Since $\bu=\bv$, $\abs{u_k-v_k}=0$ for $1\leq k\leq n$. Then $\sum_{k=1}^n\abs{u_k-v_k}=0\implies\|\bu-\bv\|=0\implies d(\bu,\bv)=0$.
                    \end{itemize}
              \item [(c)] On the left hand side, we have $d(\bu,\bw)=\|\bu-\bw\|=\sum_{k=1}^n\abs{u_k-w_k}$. On the right hand side, we have $d(\bu,\bv)+d(\bv,\bw)=\|\bu-\bv\|+\|\bv-\bw\|=\sum_{k=1}^n\abs{u_k-v_k}+\sum_{k=1}^n\abs{v_k-w_k}=\sum_{k=1}^n(\abs{u_k-v_k}+\abs{v_k-w_k})$. By triangle inequality of absolute values, $\abs{u_k-v_k}+\abs{v_k-w_k}\geq\abs{u_k-v_k+v_k-w_k}=\abs{u_k-w_k}$. So we have $\abs{u_k-v_k}+\abs{v_k-w_k}\geq\abs{u_k-w_k}$ for $1\leq k\leq n$, therefore $\sum_{k=1}^n\abs{u_k-v_k}+\sum_{k=1}^n\abs{v_k-w_k}\geq\sum_{k=1}^n\abs{u_k-w_k}\implies\|\bu-\bv\|+\|\bv-\bw\|\geq\|\bu-\bw\|\implies d(\bu,\bv)+d(\bv,\bw)\geq d(\bu,\bw)$.
          \end{itemize}
    \item [5.] Prove that $\displaystyle \lim_{p\to\infty} \|\bv\|_{p} = \|\mathbf{v}\|_{\infty}$. Recall that $\|\cdot\|_{\infty}$ is the max-norm defined as $\|\mathbf{v}\|_{\infty} := \max_{i} |v_i|$. \\
          {\em This is a hard question, so let me start you off:}
          \begin{align*}
              \|\bv\|_{p} & = \left(\sum_{i=1}^{n}|v_i|^{p}\right)^{1/p} \quad \text{ Let } |v_{i^{*}}| = \max_{i} |v_i|              \\
                          & = \left(|v_{i^{*}}|^{p}\left( 1 + \sum_{i\neq i^{*}}\frac{|v_i|^{p}}{|v_{i^{*}}|^{p}}\right)\right)^{1/p}
          \end{align*}
    \item [6.] Suppose that: $$A = LU \text{ where } L  = \left[\begin{matrix} 1 & 0 & 0 \\ \ell_{21} & 1 & 0 \\ \ell_{31} & \ell_{32} & 1 \end{matrix}\right] \text{ and } U = \left[\begin{matrix} u_{11} & u_{12} & u_{13} \\ 0 & u_{22} & u_{23} \\ 0 & 0 & u_{33} \end{matrix}\right]$$ Find an easy formula for the determinant $\det(A)$, in terms of the components of $L$ and $U$. {\em (Hint: you'll need the formula, valid for any matrices $B_1$ and $B_2$, $\det(B_1B_2) = \det(B_1)\det(B_2)$.}\\
          \textbf{Answer}: Since both $L$ and $U$ are triangular matrices, their respective determinant is simply the product of the diagonal entries. Then $\det(A)=\det(LU)=\det(L)\det(U)=u_{11}u_{22}u_{33}$.
    \item [7.] In class we defined the condition number of an invertible square matrix as $\kappa(A) = \|A\|_2\|A^{-1}\|_2$. Assume in addition that $A$ is symmetric, so that $A$ is unitarily diagonalizable. In this question you'll show that $\kappa(A) = \frac{\lambda_{\max}(A)}{\lambda_{\min}(A)}$, where $\lambda_{\max}(\cdot)$ (resp. $\lambda_{\min}(\cdot)$) denotes the largest-in-magnitude (resp. smallest-in-magnitude) eigenvalue.
          \begin{enumerate}
              \item [(a)] Show that $\|A\|_2 = \lambda_{\max}(A)$. {\em (Don't overthink this, it's essentially ``Special Case 1'' from slide 8 of Lecture 2)}.
              \item [(b)] Show that if $\lambda_1,\ldots,\lambda_n$ are the eigenvalues of $A$ then $\lambda_1^{-1},\ldots, \lambda_{n}^{-1}$ are the eigenvalues of $A^{-1}$.
              \item [(c)] Conclude that $\kappa(A) = \frac{\lambda_{\max}(A)}{\lambda_{\min}(A)}$
          \end{enumerate}
    \item [8.] Consider the problem $A\bx = \bb$ where:
          $$
              A = \left[\begin{matrix} 3 & 1 & 0 \\ 2 & 4 & 0 \\ 0 & 0 & \epsilon \end{matrix}\right] \quad \text{ and } \quad \bb = \left[\begin{matrix} 1 \\ 1 \\ 1 \end{matrix}\right]
          $$
          \begin{enumerate}
              \item Compute the condition number of $A$, $\kappa(A)$.
              \item Use Python or matlab to solve this linear system for various small value of $\epsilon$. How does the solution found by the computer change as $\epsilon$ goes to zero?
          \end{enumerate}
    \item [9.] Consider the least squares problem $\displaystyle \bx^{*} = \argmin_{\bx\in\mathbb{R}^{4}} \|A\bx - \bb\|_{2}$ where $A$ and $\bb$ are given below.
          \begin{align*}
               & A = \left[\begin{matrix} 0.7922 & 0.6787 & 0.7060 & 0.6948 \\ 0.9595 & 0.7577 & 0.0318 & 0.3171 \\ 0.6557 & 0.7431 & 0.2769 & 0.9502 \\ 0.0357 & 0.3922 & 0.0462 & 0.0344 \\ 0.8491 & 0.6555 & 0.0971 & 0.4387 \\ 0.9340 & 0.1712 & 0.8235 & 0.3816 \end{matrix} \right] \quad \bb = \left[\begin{matrix} 0.7655 \\ 0.7952 \\ 0.1869 \\ 0.4898 \\ 0.4456 \\ 0.6463 \end{matrix}\right] \\
               & Q = \left[\begin{matrix} -0.4191 & -0.1861 & -0.5979 & -0.1311 & -0.2584 & -0.5901 \\ -0.5076 & -0.1321 & 0.5495 & -0.3017 & -0.5548 & 0.1553 \\ -0.3469 & -0.4172 & -0.1468 & 0.7373 & -0.0250 & 0.3739 \\ -0.0189 & -0.5331 & -0.2732 & -0.5822 & 0.3160 & 0.4494 \\ -0.4492 & -0.0950 & 0.3924 & 0.0215 & 0.7089 & -0.3637 \\ -0.4941 & 0.6933 & -0.3005 & -0.0938 & 0.1500 & 0.3919 \end{matrix}\right]                                                      \\
               & R_1 = \left[\begin{matrix} -1.8902 & -1.3134 & -0.8595 & -1.1681 \\ 0 & -0.6892 & 0.2859 & -0.3632 \\ 0 & 0 & -0.6673 & -0.3327 \\ 0 & 0 & 0 & 0.4674 \end{matrix}\right]
          \end{align*}
          \begin{enumerate}
              \item Find $\bx^{*}$ using the QR decomposition method (Use the $Q$ and $R$ given above, which are such that $A = QR$ where $R = \left[\begin{matrix} R_1 \\ 0 \end{matrix}\right]$.). Show your work.
              \item Check your work by using matlab or python to solve the least squares problem.
          \end{enumerate}
    \item [10.] Let $A\in\mathbb{R}^{m\times n}$. Show that $\|A\|_{F} = \sqrt{\sum_{i=1}^{n}\sigma_{i}(A)^{2}}$. You will need to use the fact that $\|\cdot\|_{F}$ is unitarily invariant: $\|UA\|_{F} = \|A\|_{F}$ and $\|AV\|_{F} = \|A\|_{F}$ for all unitary matrices $U\in\mathbb{R}^{m\times m}$ and $V\in\mathbb{R}^{n\times n}$.
    \item [11.] Recall that $A^{(k)} = \sum_{i=1}^{k} \sigma_i\bu_{i}\bv_{i}^{\top}$ is the best rank $k$ approximation to $A$. Express the following quantities in terms of the singular values of $A$:
          \begin{enumerate}
              \item $\|A_{k}\|_{F}$.
              \item $\|A_{k}\|_{2}$.
              \item $\|A - A_{k}\|_{F}$.
              \item $\|A - A_{k}\|_{2}$.
          \end{enumerate}
    \item [12.] Compute the SVD of $\displaystyle A = \left[\begin{matrix} 1 & 0 \\ 2 & 7 \\ 3 & 5 \end{matrix}\right]$ by computing the eigenvalues and eigenvectors of $A^{\top}A$. Do this by hand and show your work.
    \item [13.] In class we used PCA to analyze the ``iris'' data set. In this question you will repeat the analysis for another classic machine learning data set, ``wine''. Instead of \begin{verbatim}iris = load_iris()\end{verbatim} use the command:\begin{verbatim}wine = load_wine()\end{verbatim} in the Jupyter notebook from class. Make appropriate adjustments to the code from class, and attach a plot of the loadings on to the first two singular vectors of this data set. Color the data points to indicate which of the three classes each data point belongs to.
\end{itemize}
\end{document}